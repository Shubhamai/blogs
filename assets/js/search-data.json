{
  
    
        "post0": {
            "title": "Data Exploration & Visualisations",
            "content": "# This Python 3 environment comes with many helpful analytics libraries installed # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python # For example, here&#39;s several helpful packages to load import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) # Input data files are available in the read-only &quot;../input/&quot; directory # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory import os for dirname, _, filenames in os.walk(&#39;/kaggle/input&#39;): for filename in filenames: print(os.path.join(dirname, filename)) # You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using &quot;Save &amp; Run All&quot; # You can also write temporary files to /kaggle/temp/, but they won&#39;t be saved outside of the current session . Done . !pip install -q efficientnet . # Importing Necessary Libraries %matplotlib inline import tensorflow as tf import plotly.express as px import matplotlib.pyplot as plt import seaborn as sns from tqdm.notebook import tqdm from kaggle_datasets import KaggleDatasets from collections import Counter import efficientnet.tfkeras as efn import re from tensorflow.keras import layers as L import sklearn sns.set_style(&quot;dark&quot;) sns.set(rc={&#39;figure.figsize&#39;:(12,8)}) . try: # TPU detection. No parameters necessary if TPU_NAME environment variable is # set: this is always the case on Kaggle. tpu = tf.distribute.cluster_resolver.TPUClusterResolver() print(&#39;Running on TPU &#39;, tpu.master()) except ValueError: tpu = None if tpu: tf.config.experimental_connect_to_cluster(tpu) tf.tpu.experimental.initialize_tpu_system(tpu) strategy = tf.distribute.experimental.TPUStrategy(tpu) else: # Default distribution strategy in Tensorflow. Works on CPU and single GPU. strategy = tf.distribute.get_strategy() print(&quot;REPLICAS: &quot;, strategy.num_replicas_in_sync) . Running on TPU grpc://10.0.0.2:8470 REPLICAS: 8 . # Reading the dataset dataset = pd.read_csv(&quot;../input/siim-isic-melanoma-classification/train.csv&quot;) dataset . image_name patient_id sex age_approx anatom_site_general_challenge diagnosis benign_malignant target . 0 ISIC_2637011 | IP_7279968 | male | 45.0 | head/neck | unknown | benign | 0 | . 1 ISIC_0015719 | IP_3075186 | female | 45.0 | upper extremity | unknown | benign | 0 | . 2 ISIC_0052212 | IP_2842074 | female | 50.0 | lower extremity | nevus | benign | 0 | . 3 ISIC_0068279 | IP_6890425 | female | 45.0 | head/neck | unknown | benign | 0 | . 4 ISIC_0074268 | IP_8723313 | female | 55.0 | upper extremity | unknown | benign | 0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | . 33121 ISIC_9999134 | IP_6526534 | male | 50.0 | torso | unknown | benign | 0 | . 33122 ISIC_9999320 | IP_3650745 | male | 65.0 | torso | unknown | benign | 0 | . 33123 ISIC_9999515 | IP_2026598 | male | 20.0 | lower extremity | unknown | benign | 0 | . 33124 ISIC_9999666 | IP_7702038 | male | 50.0 | lower extremity | unknown | benign | 0 | . 33125 ISIC_9999806 | IP_0046310 | male | 45.0 | torso | nevus | benign | 0 | . 33126 rows × 8 columns . Data Exploration . dataset.head() . image_name patient_id sex age_approx anatom_site_general_challenge diagnosis benign_malignant target . 0 ISIC_2637011 | IP_7279968 | male | 45.0 | head/neck | unknown | benign | 0 | . 1 ISIC_0015719 | IP_3075186 | female | 45.0 | upper extremity | unknown | benign | 0 | . 2 ISIC_0052212 | IP_2842074 | female | 50.0 | lower extremity | nevus | benign | 0 | . 3 ISIC_0068279 | IP_6890425 | female | 45.0 | head/neck | unknown | benign | 0 | . 4 ISIC_0074268 | IP_8723313 | female | 55.0 | upper extremity | unknown | benign | 0 | . dataset.isnull().sum() . image_name 0 patient_id 0 sex 65 age_approx 68 anatom_site_general_challenge 527 diagnosis 0 benign_malignant 0 target 0 dtype: int64 . dataset.describe() . age_approx target . count 33058.000000 | 33126.000000 | . mean 48.870016 | 0.017630 | . std 14.380360 | 0.131603 | . min 0.000000 | 0.000000 | . 25% 40.000000 | 0.000000 | . 50% 50.000000 | 0.000000 | . 75% 60.000000 | 0.000000 | . max 90.000000 | 1.000000 | . dataset.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 33126 entries, 0 to 33125 Data columns (total 8 columns): # Column Non-Null Count Dtype -- -- 0 image_name 33126 non-null object 1 patient_id 33126 non-null object 2 sex 33061 non-null object 3 age_approx 33058 non-null float64 4 anatom_site_general_challenge 32599 non-null object 5 diagnosis 33126 non-null object 6 benign_malignant 33126 non-null object 7 target 33126 non-null int64 dtypes: float64(1), int64(1), object(6) memory usage: 2.0+ MB . dataset.nunique() . image_name 33126 patient_id 2056 sex 2 age_approx 18 anatom_site_general_challenge 6 diagnosis 9 benign_malignant 2 target 2 dtype: int64 . Data Visualisations . f, axes = plt.subplots(2, 2, figsize=(12,12)) f.tight_layout() plt.subplots_adjust(left=0.01, wspace=0.6, hspace=0.4) sns.countplot(y=&quot;anatom_site_general_challenge&quot;, data=dataset, ax=axes[0][1]) sns.countplot(y=&quot;diagnosis&quot;, data=dataset, ax=axes[0][0]) sns.countplot(x=&#39;sex&#39;, data=dataset, ax=axes[1][0]) sns.countplot(&quot;benign_malignant&quot;, data=dataset, ax=axes[1][1]) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fa4f4725150&gt; . sns.distplot(dataset[&#39;age_approx&#39;]) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fa4f45b7a10&gt; . sns.countplot(&quot;target&quot;, data=dataset) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fa53020cf50&gt; . dataset[&#39;target&#39;].value_counts(normalize=True) * 10 . 0 9.823703 1 0.176297 Name: target, dtype: float64 . IMBALANCED DATASET! . Images Visualisations . # Showing a sample image image = plt.imread(&#39;/kaggle/input/siim-isic-melanoma-classification/jpeg/train/ISIC_5766923.jpg&#39;) plt.imshow(image) . &lt;matplotlib.image.AxesImage at 0x7fa4f42d1810&gt; . Benign Images . w = 10 h = 10 fig = plt.figure(figsize=(15, 15)) columns = 4 rows = 4 # ax enables access to manipulate each of subplots ax = [] for i in range(columns*rows): img = plt.imread(&#39;/kaggle/input/siim-isic-melanoma-classification/jpeg/train/&#39;+dataset[&#39;image_name&#39;][i]+&#39;.jpg&#39;) # create subplot and append to ax ax.append( fig.add_subplot(rows, columns, i+1) ) # Hide grid lines ax[-1].grid(False) # Hide axes ticks ax[-1].set_xticks([]) ax[-1].set_yticks([]) ax[-1].set_title(dataset[&#39;benign_malignant&#39;][i]) # set title plt.imshow(img) plt.show() # finally, render the plot . w = 10 h = 10 fig = plt.figure(figsize=(15, 15)) columns = 4 rows = 4 # ax enables access to manipulate each of subplots ax = [] for i in range(columns*rows): img = plt.imread(&#39;/kaggle/input/siim-isic-melanoma-classification/jpeg/train/&#39;+dataset.loc[dataset[&#39;target&#39;] == 1][&#39;image_name&#39;].values[i]+&#39;.jpg&#39;) # create subplot and append to ax ax.append( fig.add_subplot(rows, columns, i+1) ) # Hide grid lines ax[-1].grid(False) # Hide axes ticks ax[-1].set_xticks([]) ax[-1].set_yticks([]) ax[-1].set_title(dataset.loc[dataset[&#39;target&#39;] == 1][&#39;benign_malignant&#39;].values[i]) # set title plt.imshow(img) plt.show() # finally, render the plot . Cleaning Dataset . Removing NaN values . dataset.isnull().sum() . image_name 0 patient_id 0 sex 65 age_approx 68 anatom_site_general_challenge 527 diagnosis 0 benign_malignant 0 target 0 dtype: int64 . dataset.loc[dataset.isnull().any(axis=1)] . image_name patient_id sex age_approx anatom_site_general_challenge diagnosis benign_malignant target . 33 ISIC_0086462 | IP_3200260 | female | 30.0 | NaN | unknown | benign | 0 | . 38 ISIC_0088137 | IP_5205991 | NaN | NaN | lower extremity | unknown | benign | 0 | . 61 ISIC_0099474 | IP_3057277 | male | 45.0 | NaN | unknown | benign | 0 | . 188 ISIC_0174903 | IP_2760044 | male | 40.0 | NaN | unknown | benign | 0 | . 200 ISIC_0178744 | IP_4248414 | male | 25.0 | NaN | unknown | benign | 0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | . 32898 ISIC_9928421 | IP_0961415 | male | 45.0 | NaN | unknown | benign | 0 | . 33001 ISIC_9963692 | IP_6017204 | female | 35.0 | NaN | unknown | benign | 0 | . 33025 ISIC_9971473 | IP_1005683 | male | 70.0 | NaN | unknown | benign | 0 | . 33041 ISIC_9975949 | IP_9245079 | male | 40.0 | NaN | nevus | benign | 0 | . 33112 ISIC_9997221 | IP_6353955 | male | 55.0 | NaN | unknown | benign | 0 | . 595 rows × 8 columns . In my case, i will remove the rows with nan values, let&#39;s first try to train model with real dataset with no filled NaN value. . dataset = dataset.dropna(axis=0) dataset.isnull().sum() . image_name 0 patient_id 0 sex 0 age_approx 0 anatom_site_general_challenge 0 diagnosis 0 benign_malignant 0 target 0 dtype: int64 . Changing all column into categorical . cleaned_dataset = dataset.copy() cleaned_dataset . image_name patient_id sex age_approx anatom_site_general_challenge diagnosis benign_malignant target . 0 ISIC_2637011 | IP_7279968 | male | 45.0 | head/neck | unknown | benign | 0 | . 1 ISIC_0015719 | IP_3075186 | female | 45.0 | upper extremity | unknown | benign | 0 | . 2 ISIC_0052212 | IP_2842074 | female | 50.0 | lower extremity | nevus | benign | 0 | . 3 ISIC_0068279 | IP_6890425 | female | 45.0 | head/neck | unknown | benign | 0 | . 4 ISIC_0074268 | IP_8723313 | female | 55.0 | upper extremity | unknown | benign | 0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | . 33121 ISIC_9999134 | IP_6526534 | male | 50.0 | torso | unknown | benign | 0 | . 33122 ISIC_9999320 | IP_3650745 | male | 65.0 | torso | unknown | benign | 0 | . 33123 ISIC_9999515 | IP_2026598 | male | 20.0 | lower extremity | unknown | benign | 0 | . 33124 ISIC_9999666 | IP_7702038 | male | 50.0 | lower extremity | unknown | benign | 0 | . 33125 ISIC_9999806 | IP_0046310 | male | 45.0 | torso | nevus | benign | 0 | . 32531 rows × 8 columns . cleaned_dataset.sex = cleaned_dataset.sex.replace({&#39;male&#39;:0, &#39;female&#39;:1}) cleaned_dataset = cleaned_dataset.join(pd.get_dummies(cleaned_dataset.anatom_site_general_challenge)) cleaned_dataset = cleaned_dataset.join(pd.get_dummies(cleaned_dataset.diagnosis)) . pd.options.display.max_rows = 999 cleaned_dataset = cleaned_dataset.reset_index() cleaned_dataset.head(35) . index image_name patient_id sex age_approx anatom_site_general_challenge diagnosis benign_malignant target head/neck ... upper extremity atypical melanocytic proliferation cafe-au-lait macule lentigo NOS lichenoid keratosis melanoma nevus seborrheic keratosis solar lentigo unknown . 0 0 | ISIC_2637011 | IP_7279968 | 0 | 45.0 | head/neck | unknown | benign | 0 | 1 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | . 1 1 | ISIC_0015719 | IP_3075186 | 1 | 45.0 | upper extremity | unknown | benign | 0 | 0 | ... | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | . 2 2 | ISIC_0052212 | IP_2842074 | 1 | 50.0 | lower extremity | nevus | benign | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | . 3 3 | ISIC_0068279 | IP_6890425 | 1 | 45.0 | head/neck | unknown | benign | 0 | 1 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | . 4 4 | ISIC_0074268 | IP_8723313 | 1 | 55.0 | upper extremity | unknown | benign | 0 | 0 | ... | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | . 5 5 | ISIC_0074311 | IP_2950485 | 1 | 40.0 | lower extremity | unknown | benign | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | . 6 6 | ISIC_0074542 | IP_4698288 | 0 | 25.0 | lower extremity | unknown | benign | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | . 7 7 | ISIC_0075663 | IP_6017204 | 1 | 35.0 | torso | unknown | benign | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | . 8 8 | ISIC_0075914 | IP_7622888 | 0 | 30.0 | torso | unknown | benign | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | . 9 9 | ISIC_0076262 | IP_5075533 | 1 | 50.0 | lower extremity | unknown | benign | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | . 10 10 | ISIC_0076545 | IP_9802602 | 0 | 55.0 | upper extremity | unknown | benign | 0 | 0 | ... | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | . 11 11 | ISIC_0076742 | IP_2318163 | 0 | 75.0 | upper extremity | unknown | benign | 0 | 0 | ... | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | . 12 12 | ISIC_0076995 | IP_2235340 | 1 | 55.0 | torso | nevus | benign | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | . 13 13 | ISIC_0077472 | IP_3691360 | 1 | 40.0 | torso | unknown | benign | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | . 14 14 | ISIC_0077735 | IP_1109756 | 0 | 70.0 | torso | unknown | benign | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | . 15 15 | ISIC_0078703 | IP_7279968 | 0 | 45.0 | torso | unknown | benign | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | . 16 16 | ISIC_0078712 | IP_2189124 | 0 | 40.0 | lower extremity | unknown | benign | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | . 17 17 | ISIC_0079038 | IP_5295861 | 0 | 70.0 | torso | unknown | benign | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | . 18 18 | ISIC_0080512 | IP_1870306 | 0 | 75.0 | torso | unknown | benign | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | . 19 19 | ISIC_0080752 | IP_2613684 | 0 | 50.0 | torso | unknown | benign | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | . 20 20 | ISIC_0080817 | IP_7318404 | 0 | 50.0 | lower extremity | unknown | benign | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | . 21 21 | ISIC_0081956 | IP_2010919 | 1 | 50.0 | upper extremity | unknown | benign | 0 | 0 | ... | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | . 22 22 | ISIC_0082348 | IP_7684360 | 0 | 55.0 | torso | unknown | benign | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | . 23 23 | ISIC_0082543 | IP_9463965 | 1 | 30.0 | torso | unknown | benign | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | . 24 24 | ISIC_0082934 | IP_6572129 | 0 | 65.0 | torso | unknown | benign | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | . 25 25 | ISIC_0083035 | IP_5805281 | 0 | 50.0 | torso | unknown | benign | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | . 26 26 | ISIC_0084086 | IP_4023055 | 0 | 60.0 | lower extremity | nevus | benign | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | . 27 27 | ISIC_0084270 | IP_2961528 | 0 | 40.0 | lower extremity | nevus | benign | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | . 28 28 | ISIC_0084395 | IP_0175539 | 1 | 45.0 | torso | nevus | benign | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | . 29 29 | ISIC_0085172 | IP_1705144 | 1 | 50.0 | lower extremity | unknown | benign | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | . 30 30 | ISIC_0085718 | IP_1264754 | 1 | 65.0 | torso | unknown | benign | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | . 31 31 | ISIC_0085902 | IP_3658607 | 0 | 45.0 | torso | unknown | benign | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | . 32 32 | ISIC_0086349 | IP_2825529 | 1 | 55.0 | torso | nevus | benign | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | . 33 34 | ISIC_0086632 | IP_2114130 | 0 | 40.0 | torso | unknown | benign | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | . 34 35 | ISIC_0086709 | IP_4109313 | 0 | 30.0 | torso | unknown | benign | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | . 35 rows × 24 columns . Creating the Training &amp; Testing Dataset . A special thanks to AjayKumar for providing much an awesome Kaggle Notebook that helped me a lot to get started with tf.data . # For tf.dataset AUTO = tf.data.experimental.AUTOTUNE # Data access GCS_PATH = KaggleDatasets().get_gcs_path(&#39;siim-isic-melanoma-classification&#39;) . TRAINING_FILENAMES = tf.io.gfile.glob(GCS_PATH + &#39;/tfrecords/train*.tfrec&#39;) TEST_FILENAMES = tf.io.gfile.glob(GCS_PATH + &#39;/tfrecords/test*.tfrec&#39;) CLASSES = [0,1] IMAGE_SIZE = [1024, 1024] BATCH_SIZE = 8 * strategy.num_replicas_in_sync . def decode_image(image_data): image = tf.image.decode_jpeg(image_data, channels=3) image = tf.cast(image, tf.float32) / 255.0 # convert image to floats in [0, 1] range image = tf.reshape(image, [*IMAGE_SIZE, 3]) # explicit size needed for TPU return image def read_labeled_tfrecord(example): LABELED_TFREC_FORMAT = { &quot;image&quot;: tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring &quot;target&quot;: tf.io.FixedLenFeature([], tf.int64), # shape [] means single element } example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT) image = decode_image(example[&#39;image&#39;]) label = tf.cast(example[&#39;target&#39;], tf.int32) return image, label # returns a dataset of (image, label) pairs def read_unlabeled_tfrecord(example): UNLABELED_TFREC_FORMAT = { &quot;image&quot;: tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring &quot;image_name&quot;: tf.io.FixedLenFeature([], tf.string), # shape [] means single element # class is missing, this competitions&#39;s challenge is to predict flower classes for the test dataset } example = tf.io.parse_single_example(example, UNLABELED_TFREC_FORMAT) image = decode_image(example[&#39;image&#39;]) idnum = example[&#39;image_name&#39;] return image, idnum # returns a dataset of image(s) def load_dataset(filenames, labeled=True, ordered=False): # Read from TFRecords. For optimal performance, reading from multiple files at once and # disregarding data order. Order does not matter since we will be shuffling the data anyway. ignore_order = tf.data.Options() if not ordered: ignore_order.experimental_deterministic = False # disable order, increase speed dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO) # automatically interleaves reads from multiple files dataset = dataset.with_options(ignore_order) # uses data as soon as it streams in, rather than in its original order dataset = dataset.map(read_labeled_tfrecord if labeled else read_unlabeled_tfrecord, num_parallel_calls=AUTO) # returns a dataset of (image, label) pairs if labeled=True or (image, id) pairs if labeled=False return dataset def data_augment(image, label): # data augmentation. Thanks to the dataset.prefetch(AUTO) statement in the next function (below), # this happens essentially for free on TPU. Data pipeline code is executed on the &quot;CPU&quot; part # of the TPU while the TPU itself is computing gradients. image = tf.image.random_flip_left_right(image) image = tf.image.random_brightness(image, 0.1) image = tf.image.random_flip_up_down(image) #image = tf.image.random_saturation(image, 0, 2) return image, label def get_training_dataset(): dataset = load_dataset(TRAINING_FILENAMES, labeled=True) dataset = dataset.map(data_augment, num_parallel_calls=AUTO) dataset = dataset.repeat() # the training dataset must repeat for several epochs dataset = dataset.shuffle(2048) dataset = dataset.batch(BATCH_SIZE) dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size) return dataset def get_validation_dataset(ordered=False): dataset = load_dataset(VALIDATION_FILENAMES, labeled=True, ordered=ordered) dataset = dataset.batch(BATCH_SIZE) dataset = dataset.cache() dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size) return dataset def get_test_dataset(ordered=False): dataset = load_dataset(TEST_FILENAMES, labeled=False, ordered=ordered) dataset = dataset.batch(BATCH_SIZE) dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size) return dataset def count_data_items(filenames): # the number of data items is written in the name of the .tfrec files, i.e. flowers00-230.tfrec = 230 data items n = [int(re.compile(r&quot;-([0-9]*) .&quot;).search(filename).group(1)) for filename in filenames] return np.sum(n) NUM_TRAINING_IMAGES = count_data_items(TRAINING_FILENAMES) NUM_TEST_IMAGES = count_data_items(TEST_FILENAMES) STEPS_PER_EPOCH = NUM_TRAINING_IMAGES // BATCH_SIZE print(&#39;Dataset: {} training images and {} unlabeled test images&#39;.format(NUM_TRAINING_IMAGES,NUM_TEST_IMAGES)) . Dataset: 33126 training images and 10982 unlabeled test images . STEPS_PER_EPOCH = NUM_TRAINING_IMAGES // BATCH_SIZE EPOCHS = 5 . def build_lrfn(lr_start=0.00001, lr_max=0.0001, lr_min=0.000001, lr_rampup_epochs=20, lr_sustain_epochs=0, lr_exp_decay=.8): lr_max = lr_max * strategy.num_replicas_in_sync def lrfn(epoch): if epoch &lt; lr_rampup_epochs: lr = (lr_max - lr_start) / lr_rampup_epochs * epoch + lr_start elif epoch &lt; lr_rampup_epochs + lr_sustain_epochs: lr = lr_max else: lr = (lr_max - lr_min) * lr_exp_decay**(epoch - lr_rampup_epochs - lr_sustain_epochs) + lr_min return lr return lrfn . with strategy.scope(): efficientnetb5_model = tf.keras.Sequential([ efn.EfficientNetB5( input_shape=(*IMAGE_SIZE, 3), #weights=&#39;imagenet&#39;, weights=&#39;imagenet&#39;, include_top=False ), L.GlobalAveragePooling2D(), L.Dense(1024, activation = &#39;relu&#39;), L.Dropout(0.3), L.Dense(512, activation= &#39;relu&#39;), L.Dropout(0.2), L.Dense(256, activation=&#39;relu&#39;), L.Dropout(0.2), L.Dense(128, activation=&#39;relu&#39;), L.Dropout(0.1), L.Dense(1, activation=&#39;sigmoid&#39;) ]) . Downloading data from https://github.com/Callidior/keras-applications/releases/download/efficientnet/efficientnet-b5_weights_tf_dim_ordering_tf_kernels_autoaugment_notop.h5 115515392/115515256 [==============================] - 3s 0us/step . from tensorflow.keras import backend as K # Compatible with tensorflow backend def focal_loss(gamma=2., alpha=.25): def focal_loss_fixed(y_true, y_pred): pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred)) pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred)) return -K.mean(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1)) - K.mean((1 - alpha) * K.pow(pt_0, gamma) * K.log(1. - pt_0)) return focal_loss_fixed . efficientnetb5_model.compile( optimizer=&#39;adam&#39;, loss = focal_loss(gamma=2., alpha=.25), #loss = tf.keras.losses.BinaryCrossentropy(label_smoothing = 0.1), metrics=[&#39;binary_crossentropy&#39;, &#39;accuracy&#39;] ) efficientnetb5_model.summary() . Model: &#34;sequential&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= efficientnet-b5 (Model) (None, 32, 32, 2048) 28513520 _________________________________________________________________ global_average_pooling2d (Gl (None, 2048) 0 _________________________________________________________________ dense (Dense) (None, 1024) 2098176 _________________________________________________________________ dropout (Dropout) (None, 1024) 0 _________________________________________________________________ dense_1 (Dense) (None, 512) 524800 _________________________________________________________________ dropout_1 (Dropout) (None, 512) 0 _________________________________________________________________ dense_2 (Dense) (None, 256) 131328 _________________________________________________________________ dropout_2 (Dropout) (None, 256) 0 _________________________________________________________________ dense_3 (Dense) (None, 128) 32896 _________________________________________________________________ dropout_3 (Dropout) (None, 128) 0 _________________________________________________________________ dense_4 (Dense) (None, 1) 129 ================================================================= Total params: 31,300,849 Trainable params: 31,128,113 Non-trainable params: 172,736 _________________________________________________________________ . lrfn = build_lrfn() lr_schedule = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=1) . #model.load_weights(&#39;../input/melenoma/model_weights.h5&#39;) . history = efficientnetb5_model.fit( get_training_dataset(), epochs=EPOCHS, steps_per_epoch=STEPS_PER_EPOCH, callbacks=[lr_schedule], class_weight = {0:0.50899675,1: 28.28782609} ) . Epoch 00001: LearningRateScheduler reducing learning rate to 1e-05. Epoch 1/5 517/517 [==============================] - 579s 1s/step - accuracy: 0.9551 - loss: 0.0387 - binary_crossentropy: 0.3277 - lr: 1.0000e-05 Epoch 00002: LearningRateScheduler reducing learning rate to 4.95e-05. Epoch 2/5 517/517 [==============================] - 587s 1s/step - accuracy: 0.9824 - loss: 0.0209 - binary_crossentropy: 0.2005 - lr: 4.9500e-05 Epoch 00003: LearningRateScheduler reducing learning rate to 8.9e-05. Epoch 3/5 517/517 [==============================] - 586s 1s/step - accuracy: 0.9822 - loss: 0.0209 - binary_crossentropy: 0.1877 - lr: 8.9000e-05 Epoch 00004: LearningRateScheduler reducing learning rate to 0.0001285. Epoch 4/5 517/517 [==============================] - 586s 1s/step - accuracy: 0.9823 - loss: 0.0205 - binary_crossentropy: 0.1852 - lr: 1.2850e-04 Epoch 00005: LearningRateScheduler reducing learning rate to 0.000168. Epoch 5/5 517/517 [==============================] - 585s 1s/step - accuracy: 0.9821 - loss: 0.0201 - binary_crossentropy: 0.1807 - lr: 1.6800e-04 . # summarize history for accuracy plt.plot(history.history[&#39;loss&#39;]) plt.title(&#39;model loss&#39;) plt.ylabel(&#39;loss&#39;) plt.xlabel(&#39;epoch&#39;) plt.legend([&#39;train&#39;, &#39;val&#39;], loc=&#39;upper left&#39;) plt.show() . # summarize history for loss plt.plot(history.history[&#39;binary_crossentropy&#39;]) plt.title(&#39;model crossentropy&#39;) plt.ylabel(&#39;crossentropy&#39;) plt.xlabel(&#39;epoch&#39;) plt.legend([&#39;train&#39;, &#39;val&#39;], loc=&#39;upper left&#39;) plt.show() . efficientnetb5_model.save(&#39;complete_data_efficient_model.h5&#39;) . efficientnetb5_model.save_weights(&#39;complete_data_efficient_weights.h5&#39;) . Generating the Predictions . test_ds = get_test_dataset(ordered=True) test_images_ds = test_ds.map(lambda image, idnum: image) . probabilities = efficientnetb5_model.predict(test_images_ds) . print(&#39;Generating submission.csv file...&#39;) test_ids_ds = test_ds.map(lambda image, idnum: idnum).unbatch() test_ids = next(iter(test_ids_ds.batch(NUM_TEST_IMAGES))).numpy().astype(&#39;U&#39;) # all in one batch . Generating submission.csv file... . pred_df = pd.DataFrame({&#39;image_name&#39;: test_ids, &#39;target&#39;: np.concatenate(probabilities)}) pred_df.head() . image_name target . 0 ISIC_6381819 | 0.192496 | . 1 ISIC_5583376 | 0.157310 | . 2 ISIC_6408546 | 0.076713 | . 3 ISIC_6932354 | 0.441189 | . 4 ISIC_8191278 | 0.272053 | . sub = pd.read_csv(&quot;/kaggle/input/siim-isic-melanoma-classification/sample_submission.csv&quot;) sub . image_name target . 0 ISIC_0052060 | 0 | . 1 ISIC_0052349 | 0 | . 2 ISIC_0058510 | 0 | . 3 ISIC_0073313 | 0 | . 4 ISIC_0073502 | 0 | . ... ... | ... | . 10977 ISIC_9992485 | 0 | . 10978 ISIC_9996992 | 0 | . 10979 ISIC_9997917 | 0 | . 10980 ISIC_9998234 | 0 | . 10981 ISIC_9999302 | 0 | . 10982 rows × 2 columns . del sub[&#39;target&#39;] sub = sub.merge(pred_df, on=&#39;image_name&#39;) #sub.to_csv(&#39;submission_label_smoothing.csv&#39;, index=False) sub.to_csv(&#39;complete_data.csv&#39;, index=False) sub.head() . image_name target . 0 ISIC_0052060 | 0.085948 | . 1 ISIC_0052349 | 0.043423 | . 2 ISIC_0058510 | 0.007566 | . 3 ISIC_0073313 | 0.003228 | . 4 ISIC_0073502 | 0.204025 | . Single Image Prediction . import PIL from PIL import Image . idx = 459 print(cleaned_dataset[&#39;image_name&#39;][idx]) img = Image.open(&#39;/kaggle/input/siim-isic-melanoma-classification/jpeg/train/&#39; + cleaned_dataset[&#39;image_name&#39;][idx]+&#39;.jpg&#39;) . ISIC_0249269 . img = img.resize((1024, 1024), PIL.Image.ANTIALIAS) . img = np.array(img) img = img/255.0 . img = img[np.newaxis, ...] img.shape . (1, 1024, 1024, 3) . efficientnetb5_model.predict(img) . array([[0.01782358]], dtype=float32) .",
            "url": "https://shubhamai.github.io/blogs/2020/06/14/Melanoma-Classification.html",
            "relUrl": "/2020/06/14/Melanoma-Classification.html",
            "date": " • Jun 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Well, I love robotics and how Machine Learning &amp; AI can be applied to this thing, always seek to learn new stuff from quantum computing, space🌌, robotics 🤖 to the latest research in AI. . I have been Machine learning since more than a year, and currently, making all sorts of projects and products with Machine learning, Deep Learning, computer vision, Deep Reinforcement Learning and possibly Natural Language Processing, and of course, deploying them into the cloud for the world . I started programming way back when I was 14, playing with electronics and micro-controllers including Arduino, Raspberry PI, servo, LED 💡, and much more. Then it was time to upgrade, so I started learning Python and there was a lot of fun to learn a new programming language, I started learning Machine Learning way back when I was 15 years old, with Deep Learning, Computer Vision, Natural Language Processing and little Reinforcement Learning. . Currently, I have been working Self Paid and Teaching Assistant at Zero to Mastery. Helping students who are just getting started into Machine Learning &amp; AI. . My current goals are to learn the latest advancement in Machine Learning and learn Deep Reinforcement Learning at the end of this year. . My stuff is all around here and there in this website, and sure to join my Monthly Newsletter . My blogs and writing are available on Medium . @Shubhamai is my handle on Twitter . India is my home country and i am proud of that. .",
          "url": "https://shubhamai.github.io/blogs/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://shubhamai.github.io/blogs/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}